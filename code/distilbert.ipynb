{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "%pip install transformers datasets peft evaluate scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "%pip install jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2bc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATASET_PATH = '/Users/rumenguin/Research/MERC/EmoReact/dataset.csv'\n",
    "TRAIN_NAMES_PATH = '/Users/rumenguin/Research/MERC/EmoReact/Labels/Train_names.txt'\n",
    "VAL_NAMES_PATH = '/Users/rumenguin/Research/MERC/EmoReact/Labels/Val_names.txt'\n",
    "TEST_NAMES_PATH = '/Users/rumenguin/Research/MERC/EmoReact/Labels/Test_names.txt'\n",
    "MODEL_NAME = 'distilbert-base-uncased'  # Good balance of performance and efficiency for Mac M1\n",
    "SAVE_DIR = '/Users/rumenguin/Research/MERC/models/saved_model'\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_video_names(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Remove quotes if present and strip whitespace\n",
    "        return [line.strip().replace(\"'\", \"\").replace('\"', '') for line in f.readlines()]\n",
    "\n",
    "# Load video names for train, val, and test sets\n",
    "train_videos = load_video_names(TRAIN_NAMES_PATH)\n",
    "val_videos = load_video_names(VAL_NAMES_PATH)\n",
    "test_videos = load_video_names(TEST_NAMES_PATH)\n",
    "\n",
    "print(f\"Train videos: {len(train_videos)}\")\n",
    "print(f\"Val videos: {len(val_videos)}\")\n",
    "print(f\"Test videos: {len(test_videos)}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows to understand the data\n",
    "df.head()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Labels (emotions)\n",
    "# Convert string representation of emotion labels to lists\n",
    "def process_labels(labels):\n",
    "    if pd.isna(labels) or labels == 'None' or labels == '':\n",
    "        return []\n",
    "    try:\n",
    "        # Try to evaluate as a literal Python expression (list, tuple, etc.)\n",
    "        return eval(labels)\n",
    "    except:\n",
    "        # If it's a simple string without brackets\n",
    "        if isinstance(labels, str):\n",
    "            return [label.strip() for label in labels.split(',')]\n",
    "        return []\n",
    "\n",
    "# Apply to the Labels column\n",
    "df['Processed_Labels'] = df['Labels'].apply(process_labels)\n",
    "\n",
    "# List all possible emotions\n",
    "all_emotions = ['Curiosity', 'Uncertainty', 'Excitement', 'Happiness', 'Surprise', 'Disgust', 'Fear', 'Frustration']\n",
    "print(f\"All possible emotions: {all_emotions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Valence to numeric and handle missing values\n",
    "df['Valence'] = pd.to_numeric(df['Valence'], errors='coerce')\n",
    "# Fill missing valence with mean value\n",
    "mean_valence = df['Valence'].mean()\n",
    "df['Valence'] = df['Valence'].fillna(mean_valence)\n",
    "print(f\"Valence range: {df['Valence'].min()} to {df['Valence'].max()}\")\n",
    "\n",
    "# Create new column combining Transcript and Behavior\n",
    "df['Combined_Text'] = df['Transcript'].fillna('') + ' ' + df['Behavior'].fillna('')\n",
    "df['Combined_Text'] = df['Combined_Text'].str.strip()\n",
    "\n",
    "# Handle cases where both transcript and behavior might be empty\n",
    "df.loc[df['Combined_Text'] == '', 'Combined_Text'] = \"No information available\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e83b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, val, test based on the video files\n",
    "def get_video_name(video_path):\n",
    "    return os.path.basename(video_path)\n",
    "\n",
    "df['VideoName'] = df['Video'].apply(get_video_name)\n",
    "\n",
    "train_df = df[df['VideoName'].isin(train_videos)]\n",
    "val_df = df[df['VideoName'].isin(val_videos)]\n",
    "test_df = df[df['VideoName'].isin(test_videos)]\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Val set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=all_emotions)\n",
    "mlb.fit([all_emotions])  # Ensure all classes are represented\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels_encoded = mlb.transform(train_df['Processed_Labels'].tolist())\n",
    "val_labels_encoded = mlb.transform(val_df['Processed_Labels'].tolist())\n",
    "test_labels_encoded = mlb.transform(test_df['Processed_Labels'].tolist())\n",
    "\n",
    "# Save the class mapping\n",
    "with open(os.path.join(SAVE_DIR, 'emotion_labels.json'), 'w') as f:\n",
    "    json.dump({idx: emotion for idx, emotion in enumerate(mlb.classes_)}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187142ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, emotion_labels, valence_values, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.valence_values = valence_values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        valence = self.valence_values[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Squeeze to remove batch dimension\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'emotion_labels': torch.FloatTensor(emotion_label),\n",
    "            'valence': torch.FloatTensor([valence])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "# Define the model\n",
    "class EmotionValenceModel(nn.Module):\n",
    "    def __init__(self, n_emotions, dropout_prob=0.3):\n",
    "        super(EmotionValenceModel, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Freeze some layers of DistilBERT to reduce computational load and prevent overfitting\n",
    "        modules = [self.distilbert.embeddings, *self.distilbert.transformer.layer[:2]]\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.distilbert.config.hidden_size\n",
    "        \n",
    "        # Common layers\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.pre_classifier = nn.Linear(hidden_size, 256)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Emotion classification head\n",
    "        self.emotion_classifier = nn.Linear(256, n_emotions)\n",
    "        \n",
    "        # Valence regression head\n",
    "        self.valence_regressor = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Use [CLS] token representation\n",
    "        \n",
    "        x = self.dropout(hidden_state)\n",
    "        x = self.pre_classifier(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Multi-label emotion classification (using sigmoid for multi-label)\n",
    "        emotion_output = self.emotion_classifier(x)\n",
    "        \n",
    "        # Valence regression (value between 1-7)\n",
    "        valence_output = self.valence_regressor(x)\n",
    "        \n",
    "        return emotion_output, valence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1686115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmotionDataset(\n",
    "    train_df['Combined_Text'].tolist(),\n",
    "    train_labels_encoded,\n",
    "    train_df['Valence'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = EmotionDataset(\n",
    "    val_df['Combined_Text'].tolist(),\n",
    "    val_labels_encoded,\n",
    "    val_df['Valence'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    test_df['Combined_Text'].tolist(),\n",
    "    test_labels_encoded,\n",
    "    test_df['Valence'].tolist(),\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81275237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 16  # Smaller batch size for M1 Mac\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "n_emotions = len(all_emotions)\n",
    "model = EmotionValenceModel(n_emotions)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss functions\n",
    "emotion_criterion = nn.BCEWithLogitsLoss()\n",
    "valence_criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer with weight decay for regularization\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_epochs = 50\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, emotion_criterion, valence_criterion, num_epochs, device):\n",
    "    # For tracking metrics\n",
    "    history = {\n",
    "        'train_emotion_loss': [],\n",
    "        'train_valence_loss': [],\n",
    "        'train_total_loss': [],\n",
    "        'val_emotion_loss': [],\n",
    "        'val_valence_loss': [],\n",
    "        'val_total_loss': [],\n",
    "        'val_micro_f1': [],\n",
    "        'val_macro_f1': [],\n",
    "        'val_valence_rmse': [],\n",
    "        'val_accuracy': []  # Added accuracy tracking\n",
    "    }\n",
    "    \n",
    "    # Alpha parameter for balancing losses (can be adjusted)\n",
    "    alpha = 0.7  # Weight for emotion loss\n",
    "    beta = 0.3   # Weight for valence loss\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_emotion_loss = 0\n",
    "        train_valence_loss = 0\n",
    "        train_total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        \n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            valence = batch['valence'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            emotion_logits, valence_pred = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            e_loss = emotion_criterion(emotion_logits, emotion_labels)\n",
    "            v_loss = valence_criterion(valence_pred, valence)\n",
    "            \n",
    "            # Weighted combined loss\n",
    "            loss = alpha * e_loss + beta * v_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_emotion_loss += e_loss.item()\n",
    "            train_valence_loss += v_loss.item()\n",
    "            train_total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_description(f\"Training - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average training losses\n",
    "        avg_train_emotion_loss = train_emotion_loss / len(train_loader)\n",
    "        avg_train_valence_loss = train_valence_loss / len(train_loader)\n",
    "        avg_train_total_loss = train_total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_emotion_loss = 0\n",
    "        val_valence_loss = 0\n",
    "        val_total_loss = 0\n",
    "        \n",
    "        all_emotion_preds = []\n",
    "        all_emotion_labels = []\n",
    "        all_valence_preds = []\n",
    "        all_valence_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                valence = batch['valence'].to(device)\n",
    "                \n",
    "                emotion_logits, valence_pred = model(input_ids, attention_mask)\n",
    "                \n",
    "                e_loss = emotion_criterion(emotion_logits, emotion_labels)\n",
    "                v_loss = valence_criterion(valence_pred, valence)\n",
    "                loss = alpha * e_loss + beta * v_loss\n",
    "                \n",
    "                val_emotion_loss += e_loss.item()\n",
    "                val_valence_loss += v_loss.item()\n",
    "                val_total_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to predictions\n",
    "                emotion_preds = torch.sigmoid(emotion_logits).cpu().numpy() > 0.5  # Threshold for multi-label classification\n",
    "                # Convert lists to numpy arrays for F1 calculation\n",
    "                \n",
    "                all_emotion_preds.extend(emotion_preds.astype(int).tolist())\n",
    "                all_emotion_labels.extend(emotion_labels.cpu().numpy().astype(int).tolist())\n",
    "                all_valence_preds.extend(valence_pred.cpu().numpy().flatten().tolist())\n",
    "                all_valence_labels.extend(valence.cpu().numpy().flatten().tolist())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        avg_val_emotion_loss = val_emotion_loss / len(val_loader)\n",
    "        avg_val_valence_loss = val_valence_loss / len(val_loader)\n",
    "        avg_val_total_loss = val_total_loss / len(val_loader)\n",
    "        \n",
    "        # F1 scores for emotion classification\n",
    "        #micro_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='micro')\n",
    "        # macro_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='macro')\n",
    "        # Convert to numpy arrays\n",
    "        emotion_preds_np = np.array(all_emotion_preds)\n",
    "        emotion_labels_np = np.array(all_emotion_labels)\n",
    "\n",
    "        # F1 scores for multi-label emotion classification\n",
    "        micro_f1 = f1_score(emotion_labels_np, emotion_preds_np, average='micro', zero_division=0)\n",
    "        macro_f1 = f1_score(emotion_labels_np, emotion_preds_np, average='macro', zero_division=0)\n",
    "        \n",
    "        # RMSE for valence prediction\n",
    "        valence_rmse = np.sqrt(mean_squared_error(all_valence_labels, all_valence_preds))\n",
    "        \n",
    "        # Calculate accuracy for multi-label classification\n",
    "        accuracy = np.mean(np.array(emotion_preds_np == emotion_labels_np, dtype=float))\n",
    "        \n",
    "        # Update history\n",
    "        history['train_emotion_loss'].append(avg_train_emotion_loss)\n",
    "        history['train_valence_loss'].append(avg_train_valence_loss)\n",
    "        history['train_total_loss'].append(avg_train_total_loss)\n",
    "        history['val_emotion_loss'].append(avg_val_emotion_loss)\n",
    "        history['val_valence_loss'].append(avg_val_valence_loss)\n",
    "        history['val_total_loss'].append(avg_val_total_loss)\n",
    "        history['val_micro_f1'].append(micro_f1)\n",
    "        history['val_macro_f1'].append(macro_f1)\n",
    "        history['val_valence_rmse'].append(valence_rmse)\n",
    "        history['val_accuracy'].append(accuracy)  # Added accuracy to history\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train - Emotion Loss: {avg_train_emotion_loss:.4f}, Valence Loss: {avg_train_valence_loss:.4f}, Total Loss: {avg_train_total_loss:.4f}\")\n",
    "        print(f\"Val - Emotion Loss: {avg_val_emotion_loss:.4f}, Valence Loss: {avg_val_valence_loss:.4f}, Total Loss: {avg_val_total_loss:.4f}\")\n",
    "        print(f\"Val - Micro F1: {micro_f1:.4f}, Macro F1: {macro_f1:.4f}, Valence RMSE: {valence_rmse:.4f}, Accuracy: {accuracy:.4f}\")  # Added accuracy to print statement\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_total_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_total_loss\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'dbert_best_model.pt'))\n",
    "            print(\"Saved best model!\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'dbert_final_model.pt'))\n",
    "    \n",
    "    # Save training history\n",
    "    with open(os.path.join(SAVE_DIR, 'dbert_training_history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    emotion_criterion,\n",
    "    valence_criterion,\n",
    "    num_epochs,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e622211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # Plot the training history\n",
    "def plot_training_history(history):\n",
    "    # Create subplots - changed to 2x3 grid to add accuracy plot\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Plot total loss\n",
    "    axs[0, 0].plot(history['train_total_loss'], label='Train Loss')\n",
    "    axs[0, 0].plot(history['val_total_loss'], label='Val Loss')\n",
    "    axs[0, 0].set_title('Total Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Plot emotion loss\n",
    "    axs[0, 1].plot(history['train_emotion_loss'], label='Train Emotion Loss')\n",
    "    axs[0, 1].plot(history['val_emotion_loss'], label='Val Emotion Loss')\n",
    "    axs[0, 1].set_title('Emotion Loss')\n",
    "    axs[0, 1].set_xlabel('Epoch')\n",
    "    axs[0, 1].set_ylabel('Loss')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # Plot valence loss and RMSE\n",
    "    axs[0, 2].plot(history['train_valence_loss'], label='Train Valence Loss')\n",
    "    axs[0, 2].plot(history['val_valence_loss'], label='Val Valence Loss')\n",
    "    axs[0, 2].plot(history['val_valence_rmse'], label='Val Valence RMSE')\n",
    "    axs[0, 2].set_title('Valence Loss & RMSE')\n",
    "    axs[0, 2].set_xlabel('Epoch')\n",
    "    axs[0, 2].set_ylabel('Loss / RMSE')\n",
    "    axs[0, 2].legend()\n",
    "    \n",
    "    # Plot F1 scores\n",
    "    axs[1, 0].plot(history['val_micro_f1'], label='Micro F1')\n",
    "    axs[1, 0].plot(history['val_macro_f1'], label='Macro F1')\n",
    "    axs[1, 0].set_title('F1 Scores')\n",
    "    axs[1, 0].set_xlabel('Epoch')\n",
    "    axs[1, 0].set_ylabel('F1 Score')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axs[1, 1].plot(history['val_accuracy'], label='Accuracy', color='purple')\n",
    "    axs[1, 1].set_title('Model Accuracy')\n",
    "    axs[1, 1].set_xlabel('Epoch')\n",
    "    axs[1, 1].set_ylabel('Accuracy')\n",
    "    axs[1, 1].set_ylim([0, 1])  # Accuracy is between 0 and 1\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    # Hide the unused subplot\n",
    "    axs[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'dbert_training_history.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader, emotion_criterion, valence_criterion, device):\n",
    "    model.eval()\n",
    "    test_emotion_loss = 0\n",
    "    test_valence_loss = 0\n",
    "    all_emotion_preds = []\n",
    "    all_emotion_labels = []\n",
    "    all_valence_preds = []\n",
    "    all_valence_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            valence = batch['valence'].to(device)\n",
    "            \n",
    "            emotion_logits, valence_pred = model(input_ids, attention_mask)\n",
    "            \n",
    "            e_loss = emotion_criterion(emotion_logits, emotion_labels)\n",
    "            v_loss = valence_criterion(valence_pred, valence)\n",
    "            \n",
    "            test_emotion_loss += e_loss.item()\n",
    "            test_valence_loss += v_loss.item()\n",
    "            \n",
    "            # Convert logits to predictions\n",
    "            emotion_preds = torch.sigmoid(emotion_logits).cpu().numpy() > 0.5 \n",
    "            \n",
    "            all_emotion_preds.extend(emotion_preds.astype(int).tolist())\n",
    "            all_emotion_labels.extend(emotion_labels.cpu().numpy().astype(int).tolist())\n",
    "            all_valence_preds.extend(valence_pred.cpu().numpy().flatten().tolist())\n",
    "            all_valence_labels.extend(valence.cpu().numpy().flatten().tolist())\n",
    "            \n",
    "    # Calculate test metrics\n",
    "    avg_test_emotion_loss = test_emotion_loss / len(test_loader)\n",
    "    avg_test_valence_loss = test_valence_loss / len(test_loader)\n",
    "    \n",
    "    # F1 scores for emotion classification\n",
    "    # Convert to numpy arrays\n",
    "    emotion_preds_np = np.array(all_emotion_preds)\n",
    "    emotion_labels_np = np.array(all_emotion_labels)\n",
    "    \n",
    "    # F1 scores for multi-label emotion classification\n",
    "    micro_f1 = f1_score(emotion_labels_np, emotion_preds_np, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(emotion_labels_np, emotion_preds_np, average='macro', zero_division=0)\n",
    "    \n",
    "    # F1 scores for each class\n",
    "    class_f1 = f1_score(all_emotion_labels, all_emotion_preds, average=None)\n",
    "    class_report = {emotion: score for emotion, score in zip(all_emotions, class_f1)}\n",
    "    \n",
    "    # RMSE for valence prediction\n",
    "    valence_rmse = np.sqrt(mean_squared_error(all_valence_labels, all_valence_preds))\n",
    "    \n",
    "    # Calculate accuracy for multi-label classification\n",
    "    accuracy = np.mean(np.array(emotion_preds_np == emotion_labels_np, dtype=float))\n",
    "    \n",
    "    # Create a confusion matrix for multi-label is complex\n",
    "    # Instead, analyze per-class precision, recall\n",
    "    results = {\n",
    "        'test_emotion_loss': avg_test_emotion_loss,\n",
    "        'test_valence_loss': avg_test_valence_loss,\n",
    "        'test_micro_f1': micro_f1,\n",
    "        'test_macro_f1': macro_f1,\n",
    "        'test_valence_rmse': valence_rmse,\n",
    "        'test_accuracy': accuracy,  # Added accuracy\n",
    "        'class_f1': class_report\n",
    "    }\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"Emotion Loss: {avg_test_emotion_loss:.4f}\")\n",
    "    print(f\"Valence Loss: {avg_test_valence_loss:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Valence RMSE: {valence_rmse:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")  # Added accuracy print statement\n",
    "    \n",
    "    # Save results\n",
    "    with open(os.path.join(SAVE_DIR, 'dbert_test_results.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'dbert_best_model.pt')))\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = evaluate_model(model, test_loader, emotion_criterion, valence_criterion, device)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Print test results\n",
    "'''A\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Emotion Loss: {test_results['test_emotion_loss']:.4f}\")\n",
    "print(f\"Valence Loss: {test_results['test_valence_loss']:.4f}\")\n",
    "print(f\"Micro F1: {test_results['test_micro_f1']:.4f}\")\n",
    "print(f\"Macro F1: {test_results['test_macro_f1']:.4f}\")\n",
    "print(f\"Valence RMSE: {test_results['test_valence_rmse']:.4f}\")\n",
    "print(f\"Accuracy: {test_results['test_accuracy']:.4f}\")\n",
    "'''\n",
    "print(\"\\nClass-wise F1 Scores:\")\n",
    "for emotion, f1 in test_results['class_f1'].items():\n",
    "    print(f\"{emotion}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save radar chart\n",
    "def create_emotion_radar_chart(class_f1, emotions, save_dir):\n",
    "    # Number of variables\n",
    "    N = len(emotions)\n",
    "    \n",
    "    # Get F1 scores in same order as emotions\n",
    "    f1_scores = [class_f1[emotion] for emotion in emotions]\n",
    "    \n",
    "    # What will be the angle of each axis in the plot\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # F1 scores for plotting (add first element at end to close the loop)\n",
    "    values = f1_scores.copy()\n",
    "    values += values[:1]  \n",
    "    \n",
    "    # Initialize the figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], emotions, fontsize=12)\n",
    "    \n",
    "    # Draw the y-axis labels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], \n",
    "               fontsize=10, color=\"grey\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', color='#FF7F0E')\n",
    "    \n",
    "    # Fill area\n",
    "    ax.fill(angles, values, alpha=0.25, color='#FF7F0E')\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(\"Emotion-wise F1 Scores\", size=20, y=1.05)\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'dbert_emotion_F1s.png'))\n",
    "    plt.show()\n",
    "create_emotion_radar_chart(test_results[\"class_f1\"], all_emotions, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions for a new input\n",
    "def predict_emotions_and_valence(text, model, tokenizer, mlb, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emotion_logits, valence_pred = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Convert logits to predictions\n",
    "        emotion_probs = torch.sigmoid(emotion_logits).cpu().numpy()\n",
    "        emotion_preds = emotion_probs > 0.5\n",
    "        \n",
    "        # Get the predicted emotions\n",
    "        predicted_emotions = mlb.inverse_transform(emotion_preds)[0]\n",
    "        \n",
    "        # Get the predicted valence\n",
    "        predicted_valence = valence_pred.cpu().numpy().item()\n",
    "        \n",
    "        # Get the probabilities for each emotion\n",
    "        emotion_probabilities = {emotion: prob for emotion, prob in zip(mlb.classes_, emotion_probs[0])}\n",
    "    \n",
    "    return predicted_emotions, predicted_valence, emotion_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction function\n",
    "test_text = \"I am really excited about this new movie. The trailer looks amazing and I can't wait to see it!\"\n",
    "predicted_emotions, predicted_valence, emotion_probs = predict_emotions_and_valence(\n",
    "    test_text, model, tokenizer, mlb, device\n",
    ")\n",
    "\n",
    "print(\"\\nExample Prediction:\")\n",
    "print(f\"Input text: {test_text}\")\n",
    "print(f\"Predicted emotions: {predicted_emotions}\")\n",
    "print(f\"Predicted valence: {predicted_valence:.2f}\")\n",
    "print(\"Emotion probabilities:\")\n",
    "for emotion, prob in emotion_probs.items():\n",
    "    print(f\"{emotion}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b83495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the complete model, tokenizer, and configuration\n",
    "model_save_path = os.path.join(SAVE_DIR, 'dbert')\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'n_emotions': len(all_emotions),\n",
    "    'emotions': all_emotions,\n",
    "    'valence_range': [df['Valence'].min(), df['Valence'].max()],\n",
    "    'max_length': 512,\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_save_path, 'config.json'), 'w') as f:\n",
    "    json.dump(model_config, f)\n",
    "\n",
    "# Save the model state\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, 'model_state.pt'))\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"\\nModel, tokenizer, and configuration saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ae713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final messages\n",
    "print(\"\\nTraining completed successfully!\")\n",
    "print(f\"Best model saved to: {os.path.join(SAVE_DIR, 'dbert_best_model.pt')}\")\n",
    "print(f\"Final model saved to: {os.path.join(SAVE_DIR, 'dbert_final_model.pt')}\")\n",
    "print(f\"Complete model package saved to: {model_save_path}\")\n",
    "print(f\"Training history plot saved to: {os.path.join(SAVE_DIR, 'dbert_training_history.png')}\")\n",
    "print(f\"Training history saved to: {os.path.join(SAVE_DIR, 'dbert_training_history.json')}\")\n",
    "print(f\"Test results saved to: {os.path.join(SAVE_DIR, 'dbert_test_results.json')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
